{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd6d031-26c1-4b2f-b085-f863c7bbbaf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 53800)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/accumulators.py\", line 299, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/accumulators.py\", line 271, in poll\n",
      "    if self.rfile in r and func():\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/accumulators.py\", line 275, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/serializers.py\", line 597, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"pyspark_jupyter\").master(\"spark://spark-master:7077\").getOrCreate()\n",
    "spark\n",
    "data = [(\"Java\", \"20000\"), (\"Python\", \"100000\"), (\"Scala\", \"3000\")]\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()\n",
    "\n",
    "df1 = spark.read.csv(\"/opt/spark/data/AAPL.csv\")\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aeba383-e7a9-4a33-ab1a-56926fad0ada",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import boto3\n",
    "\n",
    "# Initialize Spark session with the necessary Hadoop AWS package\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark with LocalStack S3\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localstack:4566\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"test\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"test\") \\\n",
    "    .config(\"spark.jars\", \"/usr/local/lib/python3.10/dist-packages/pyspark/jars/hadoop-aws-3.2.0.jar,/usr/local/lib/python3.10/dist-packages/pyspark/jars/aws-java-sdk-bundle-1.11.375.jar,/usr/local/lib/python3.10/dist-packages/pyspark/jars/hadoop-common-3.2.0.jar\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "    \n",
    "spark\n",
    "\n",
    "s3 = boto3.client('s3', endpoint_url=\"http://localstack:4566\", \n",
    "                  aws_access_key_id=\"test\", aws_secret_access_key=\"test\", region_name=\"us-east-1\")\n",
    "\n",
    "# Create a bucket in LocalStack's S3\n",
    "bucket_name = 'my-test-bucket'\n",
    "s3.create_bucket(Bucket=bucket_name)\n",
    "\n",
    "# Upload a file to S3\n",
    "# s3.put_object(Bucket=bucket_name, Key=\"test_file.txt\", Body=\"This is a test file.\")\n",
    "\n",
    "# # Read from the bucket using PySpark\n",
    "# df = spark.read.text(f\"s3a://{bucket_name}/test_file.txt\")\n",
    "df.show()\n",
    "s3.list_buckets()\n",
    "s3.upload_file(\"/opt/spark/data/AAPL.csv\", 'my-test-bucket', 'AAPL')\n",
    "\n",
    "objects = s3.list_objects_v2(Bucket='my-test-bucket')\n",
    "\n",
    "for obj in objects['Contents']:\n",
    "    print(obj['Key'])\n",
    "# Stop Spark session\n",
    "#spark.stop()\n",
    "csv = spark.read.text(\"s3a://my-test-bucket/AAPL.csv\")\n",
    "csv.show()\n",
    "\n",
    "response = s3.get_object(Bucket='my-test-bucket', Key='AAPL')\n",
    "\n",
    "object_content = response['Body'].read().decode('utf-8')\n",
    "\n",
    "print(object_content)\n",
    "\n",
    "\n",
    "    #.config(\"spark.jars\", \"/usr/local/lib/python3.10/dist-packages/pyspark/jars/hadoop-aws-3.3.4.jar,/usr/local/lib/python3.10/dist-packages/pyspark/jars/aws-java-sdk-bundle-2.33.13.jar,/usr/local/lib/python3.10/dist-packages/pyspark/jars/hadoop-common-3.3.4.jar\") \\\n",
    "    #.config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "\n",
    "\n",
    "\n",
    " # .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    " #    .config(\"spark.hadoop.fs.s3a.connection.maximum\", \"100\") \\\n",
    " #    .config(\"spark.hadoop.fs.s3a.connection.timeout\", \"60000\") \\\n",
    " #    .config(\"spark.hadoop.fs.s3a.connection.acquisition.timeout\", \"60000\") \\\n",
    " #    .config(\"spark.hadoop.fs.s3a.read.timeout\", \"120000\") \\\n",
    " #    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    " #    .config(\"spark.jars\", \"/usr/local/lib/python3.10/dist-packages/pyspark/jars/hadoop-aws-3.2.0.jar,/usr/local/lib/python3.10/dist-packages/pyspark/jars/aws-java-sdk-bundle-1.11.375.jar,/usr/local/lib/python3.10/dist-packages/pyspark/jars/hadoop-common-3.2.0.jar\") \\\n",
    " #    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5b58e8-59f0-4bb6-a649-892aeff4af9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #.config(\"spark.jars\", \"/usr/local/lib/python3.10/dist-packages/pyspark/jars/hadoop-aws-3.3.4.jar,/usr/local/lib/python3.10/dist-packages/pyspark/jars/aws-java-sdk-bundle-2.33.13.jar,/usr/local/lib/python3.10/dist-packages/pyspark/jars/hadoop-common-3.3.4.jar\") \\\n",
    "\n",
    "\n",
    "! ls /usr/local/lib/python3.10/dist-packages/pyspark/jars/hadoop-aws-3.2.0.jar\n",
    "\n",
    "! ls /usr/local/lib/python3.10/dist-packages/pyspark/jars/aws-java-sdk-bundle-1.11.375.jar\n",
    "\n",
    "! ls /usr/local/lib/python3.10/dist-packages/pyspark/jars/hadoop-common-3.2.0.jar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3e19b8-b8a1-49d6-b731-c32878712764",
   "metadata": {},
   "outputs": [],
   "source": [
    "4709 creekstone drive, 27703 Duraham 280\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "969771d5-ced1-451c-bcb4-3b8393f01b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"services\": {\"acm\": \"disabled\", \"apigateway\": \"disabled\", \"cloudformation\": \"disabled\", \"cloudwatch\": \"disabled\", \"config\": \"disabled\", \"dynamodb\": \"available\", \"dynamodbstreams\": \"available\", \"ec2\": \"disabled\", \"es\": \"disabled\", \"events\": \"disabled\", \"firehose\": \"disabled\", \"iam\": \"disabled\", \"kinesis\": \"available\", \"kms\": \"disabled\", \"lambda\": \"available\", \"logs\": \"disabled\", \"opensearch\": \"disabled\", \"redshift\": \"disabled\", \"resource-groups\": \"disabled\", \"resourcegroupstaggingapi\": \"disabled\", \"route53\": \"disabled\", \"route53resolver\": \"disabled\", \"s3\": \"available\", \"s3control\": \"disabled\", \"scheduler\": \"disabled\", \"secretsmanager\": \"disabled\", \"ses\": \"disabled\", \"sns\": \"disabled\", \"sqs\": \"disabled\", \"ssm\": \"disabled\", \"stepfunctions\": \"disabled\", \"sts\": \"available\", \"support\": \"disabled\", \"swf\": \"disabled\", \"transcribe\": \"disabled\"}, \"edition\": \"community\", \"version\": \"4.8.2.dev13\"}"
     ]
    }
   ],
   "source": [
    "! curl http://localstack:4566/_localstack/health "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
