x-spark-comon:
  &spark-common
  build:
    context: .
    dockerfile: Dockerfile.spark
  volumes:
    - ./jobs:/opt/spark/jobs
    - ./dags:/opt/spark/dags
    - ./notebooks:/home/spark/notebooks
    - ./data:/opt/spark/data:rw

services:
  spark-master:
    #image: spark:4.0.0-scala2.13-java21-python3-ubuntu
    # build:
    #   context: .
    #   dockerfile: Dockerfile.jupyter
    <<: *spark-common

    environment:
      - SPARK_MASTER_HOST=spark-master # Or the container's IP/hostname
      - SPARK_MASTER_PORT=7077
      - SPARK_MASTER_WEBUI_PORT=8080
      - SPARK_NO_DAEMONIZE=true # To run master in foreground
    ports:
      - 7077:7077
      - 8080:8080
      - 8888:8888
    volumes:
      - ./data:/opt/spark/data
    container_name: spark-master
    command: "/opt/spark/sbin/start-master.sh"
    # && nohup jupyter lab --ip=0.0.0.0 --allow-root --no-browser --NotebookApp.token='' &"
        
    networks:
      elt:
        ipv4_address: 172.25.0.2


  spark-worker:
      #image: spark:4.0.0-scala2.13-java21-python3-ubuntu
      # build:
      #   context: .
      #   dockerfile: Dockerfile.jupyter
      <<: *spark-common
      environment:
        - SPARK_MASTER_HOST=spark-master # Or the container's IP/hostname
        - SPARK_MASTER_PORT=7077
        - SPARK_MASTER_WEBUI_PORT=8080
        - SPARK_NO_DAEMONIZE=true # To run master in foreground
        - SPARK_MASTER_URL=spark://spark-master:7077
      container_name: spark-worker
      command: "/opt/spark/sbin/start-worker.sh spark://spark-master:7077"    
      # volumes:
      #   - ./data:/opt/spark/data
      depends_on:
      - spark-master
      networks:
        elt:
          ipv4_address: 172.25.0.3

  spark-history:
      #image: spark:4.0.0-scala2.13-java21-python3-ubuntu
      # build:
      #   context: .
      #   dockerfile: Dockerfile.jupyter
      <<: *spark-common
      environment:
        - SPARK_MASTER_HOST=spark-master # Or the container's IP/hostname
        - SPARK_MASTER_PORT=7077
        - SPARK_MASTER_WEBUI_PORT=8080
        - SPARK_NO_DAEMONIZE=true # To run master in foreground
        - SPARK_MASTER_URL=spark://spark-master:7077
      container_name: spark-history
      command: "/opt/spark/sbin/start-history-server.sh"    
      ports:
        - 18080:18080
      volumes:
      - spark-events:/tmp/spark-events 
      #- ./data:/opt/spark/data
      depends_on:
        - spark-master    
      networks:
        elt:
          ipv4_address: 172.25.0.4

  postgres:
    image: postgres:13
    container_name: postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 10s
      retries: 5
      start_period: 5s
    restart: always
    networks:
      elt:
        ipv4_address: 172.25.0.5
  localstack:
      image: localstack/localstack:latest
      environment:
        - SERVICES=s3,lambda,dynamodb # Specify the services you need
        - DEFAULT_REGION=us-east-1
        - DOCKER_HOST=unix:///var/run/docker.sock # Required for Lambda functions
      container_name: localstack
      ports:
        - "4510-4559:4510-4559" # For specific services if needed
        - "4566:4566" # Default LocalStack port
        - "8090:8080" # LocalStack Web Application
      volumes:
        - ./aws:/etc/localstack/init/ready.d
        - ./data:/data
        - "/var/run/docker.sock:/var/run/docker.sock" # For Lambda functions
      networks:
        elt:
          ipv4_address: 172.25.0.6
  jupyter:
  #image: spark:4.0.0-scala2.13-java21-python3-ubuntu
    build:
      context: .
      dockerfile: Dockerfile.jupyter
    user: 
      root
    environment:
      - SPARK_MASTER_HOST=spark-master # Or the container's IP/hostname
      - SPARK_MASTER_PORT=7077
      - SPARK_MASTER_WEBUI_PORT=8080
      - SPARK_NO_DAEMONIZE=true # To run master in foreground
      - SPARK_MASTER_URL=spark://spark-master:7079
    ports:
      - 8889:8888
    container_name: jupyter-lb
    command: "jupyter lab --ip=0.0.0.0 --no-browser  --allow-root --NotebookApp.token='' --NotebookApp.password='' --notebook-dir=/home/jupyter/notebooks"
    depends_on:
      - spark-master    
    volumes:
      - ./data:/opt/spark/data
      - ./notebooks:/home/jupyter/notebooks
    networks:
        elt:
          ipv4_address: 172.25.0.7
volumes:
  postgres-db-volume:
  spark-events:
networks:
  elt:
    ipam:
      config:
        - subnet: 172.25.0.0/16